# 🧠📌 Word Embeddings y Word2Vec

✅ Este módulo reúne prácticas y experimentos utilizando técnicas de representación vectorial del lenguaje, enfocándonos en **Word Embeddings** y el modelo **Word2Vec**.

---

## 🔍 Contenido

- 🔤 Tokenización y preparación del corpus textual  
- ⚙️ Entrenamiento de modelos Word2Vec (CBOW y Skip-Gram)  
- 📏 Evaluación de similitud semántica entre palabras  
- 📈 Visualización de vectores en espacios reducidos (PCA / t-SNE)  
- 📊 Comparación con otras técnicas de representación (Bag of Words, TF-IDF)  
- 🧠 Exploración semántica y analógica: *"rey - hombre + mujer = reina"*

🐍 Desarrollado en **Python**, utilizando librerías como:  
`gensim`, `spaCy`, `scikit-learn`, `matplotlib`, `seaborn`, `pandas`

---

## 🚀 Habilidades desarrolladas

- 🧠 Comprensión profunda de cómo las palabras se representan como vectores  
- 🔍 Análisis semántico y cálculo de similitudes  
- 🧪 Implementación de modelos de aprendizaje no supervisado sobre texto  
- 📊 Visualización en 2D de espacios vectoriales de palabras

---

## 🎯 Objetivo general

Aplicar modelos de representación vectorial del lenguaje para capturar relaciones semánticas y sintácticas entre palabras, y visualizar estructuras del lenguaje en espacios multidimensionales mediante el uso de **Word2Vec**.
