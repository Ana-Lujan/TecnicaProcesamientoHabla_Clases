# ğŸ§ ğŸ“Œ Word Embeddings y Word2Vec

âœ… Este mÃ³dulo reÃºne prÃ¡cticas y experimentos utilizando tÃ©cnicas de representaciÃ³n vectorial del lenguaje, enfocÃ¡ndonos en **Word Embeddings** y el modelo **Word2Vec**.

---

## ğŸ” Contenido

- ğŸ”¤ TokenizaciÃ³n y preparaciÃ³n del corpus textual  
- âš™ï¸ Entrenamiento de modelos Word2Vec (CBOW y Skip-Gram)  
- ğŸ“ EvaluaciÃ³n de similitud semÃ¡ntica entre palabras  
- ğŸ“ˆ VisualizaciÃ³n de vectores en espacios reducidos (PCA / t-SNE)  
- ğŸ“Š ComparaciÃ³n con otras tÃ©cnicas de representaciÃ³n (Bag of Words, TF-IDF)  
- ğŸ§  ExploraciÃ³n semÃ¡ntica y analÃ³gica: *"rey - hombre + mujer = reina"*

ğŸ Desarrollado en **Python**, utilizando librerÃ­as como:  
`gensim`, `spaCy`, `scikit-learn`, `matplotlib`, `seaborn`, `pandas`

---

## ğŸš€ Habilidades desarrolladas

- ğŸ§  ComprensiÃ³n profunda de cÃ³mo las palabras se representan como vectores  
- ğŸ” AnÃ¡lisis semÃ¡ntico y cÃ¡lculo de similitudes  
- ğŸ§ª ImplementaciÃ³n de modelos de aprendizaje no supervisado sobre texto  
- ğŸ“Š VisualizaciÃ³n en 2D de espacios vectoriales de palabras

---

## ğŸ¯ Objetivo general

Aplicar modelos de representaciÃ³n vectorial del lenguaje para capturar relaciones semÃ¡nticas y sintÃ¡cticas entre palabras, y visualizar estructuras del lenguaje en espacios multidimensionales mediante el uso de **Word2Vec**.
