# 🤖🧠 Arquitecturas: Transformers

✅ Este módulo aborda el estudio de las **arquitecturas Transformers**, fundamentales en los modelos modernos de lenguaje natural como BERT, GPT y otros modelos de gran escala.

---

## 🔍 Contenido

- 🔍 Introducción al mecanismo de atención (Self-Attention)  
- 🧠 Arquitectura base del Transformer: Encoder & Decoder  
- 📚 Modelos preentrenados: BERT, GPT, T5  
- ⚙️ Transfer learning y fine-tuning en tareas de PLN  
- 🧪 Aplicación a tareas como clasificación, generación y traducción de texto  
- 💡 Implementación con `transformers` de Hugging Face  

🐍 Desarrollado en **Python**, utilizando librerías como:  
`transformers`, `datasets`, `torch`, `tensorflow`, `pandas`, `matplotlib`

---

## 🚀 Habilidades desarrolladas

- 🧠 Comprensión del funcionamiento interno de un Transformer  
- 🎯 Aplicación de modelos preentrenados a tareas específicas de lenguaje  
- 🔁 Fine-tuning y evaluación de desempeño  
- 📊 Interpretación de resultados en tareas de generación, análisis o clasificación

---

## 🎯 Objetivo general

Entender el rol de las **arquitecturas Transformer** en el procesamiento del lenguaje actual y aplicar modelos de última generación para resolver tareas de PLN con técnicas de transferencia de aprendizaje.

