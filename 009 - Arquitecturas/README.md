# ğŸ¤–ğŸ§  Arquitecturas: Transformers

âœ… Este mÃ³dulo aborda el estudio de las **arquitecturas Transformers**, fundamentales en los modelos modernos de lenguaje natural como BERT, GPT y otros modelos de gran escala.

---

## ğŸ” Contenido

- ğŸ” IntroducciÃ³n al mecanismo de atenciÃ³n (Self-Attention)  
- ğŸ§  Arquitectura base del Transformer: Encoder & Decoder  
- ğŸ“š Modelos preentrenados: BERT, GPT, T5  
- âš™ï¸ Transfer learning y fine-tuning en tareas de PLN  
- ğŸ§ª AplicaciÃ³n a tareas como clasificaciÃ³n, generaciÃ³n y traducciÃ³n de texto  
- ğŸ’¡ ImplementaciÃ³n con `transformers` de Hugging Face  

ğŸ Desarrollado en **Python**, utilizando librerÃ­as como:  
`transformers`, `datasets`, `torch`, `tensorflow`, `pandas`, `matplotlib`

---

## ğŸš€ Habilidades desarrolladas

- ğŸ§  ComprensiÃ³n del funcionamiento interno de un Transformer  
- ğŸ¯ AplicaciÃ³n de modelos preentrenados a tareas especÃ­ficas de lenguaje  
- ğŸ” Fine-tuning y evaluaciÃ³n de desempeÃ±o  
- ğŸ“Š InterpretaciÃ³n de resultados en tareas de generaciÃ³n, anÃ¡lisis o clasificaciÃ³n

---

## ğŸ¯ Objetivo general

Entender el rol de las **arquitecturas Transformer** en el procesamiento del lenguaje actual y aplicar modelos de Ãºltima generaciÃ³n para resolver tareas de PLN con tÃ©cnicas de transferencia de aprendizaje.

